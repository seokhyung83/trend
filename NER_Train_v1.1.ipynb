{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aae8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql, os, copy, json, time, openpyxl, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import  Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c22703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_generate(sent, tok, MAX_LEN):\n",
    "    encode_dict = tok.encode_plus(text=sent, \n",
    "                                    add_special_tokens=True, max_length=MAX_LEN,\n",
    "                                    return_token_type_ids=True, padding='max_length', \n",
    "                                    return_attention_mask=True, truncation=True) \n",
    "    input_id = encode_dict['input_ids']\n",
    "    attention_mask = encode_dict['attention_mask']\n",
    "    token_type_id = encode_dict['token_type_ids']\n",
    "    return input_id, attention_mask, token_type_id  \n",
    "\n",
    "def convert_label(words, labels_idx, tok,  ner_b_label, max_seq_len):\n",
    "    cls_token = tok.cls_token\n",
    "    sep_token = tok.sep_token\n",
    "    unk_token = tok.unk_token\n",
    "    pad_token_id = tok.pad_token_id\n",
    "    tokens = []\n",
    "    labels_ids = []    \n",
    "    for word, slot_label in zip(words, labels_idx):\n",
    "        w_token = tok.tokenize(word)\n",
    "        if not w_token:\n",
    "            w_token = [unk_token]\n",
    "        tokens.extend(w_token)\n",
    "        if int(slot_label) in ner_b_label:\n",
    "            labels_ids.extend([int(slot_label)] + [int(slot_label) + 1] * (len(w_token)-1))\n",
    "        else:\n",
    "            labels_ids.extend([int(slot_label)] * len(w_token))            \n",
    "    special_tokens_cnt = 2\n",
    "    if len(labels_ids) > max_seq_len - special_tokens_cnt:\n",
    "        labels_ids = labels_ids[:(max_seq_len - special_tokens_cnt)]        \n",
    "    labels_ids += [sep_token_label_id]\n",
    "    labels_ids = [cls_token_label_id] + labels_ids    \n",
    "    padding_len = max_seq_len - len(labels_ids)\n",
    "    labels_ids = labels_ids + ([pad_token_label_id] * padding_len)    \n",
    "    return labels_ids\n",
    "\n",
    "def generate_input(df, tok, ner_b_label, max_seq_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_list = []    \n",
    "    for i, data in enumerate(df[['sentence', 'label']].values):\n",
    "        sentence, labels = data\n",
    "        words = sentence.split()\n",
    "        labels = labels.split()\n",
    "        labels_idx = []        \n",
    "        for l in labels:    \n",
    "            labels_idx.append(train_label_l1.index(l+'-B') if l+'-B' in train_label_l1 else train_label_l1.index(\"UNK\"))        \n",
    "        input_id, attention_mask, token_type_id = token_generate(sentence, tok, max_seq_len)\n",
    "        convert_label_id = convert_label(words, labels_idx, tok, ner_b_label, max_seq_len)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_list.append(convert_label_id)        \n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_list = np.asarray(label_list, dtype=int)\n",
    "    inputs = (input_ids, attention_masks, token_type_ids)    \n",
    "    return inputs, label_list\n",
    "\n",
    "class custom_set(Dataset):\n",
    "    def __init__(self, dX, dY=None):\n",
    "        self.input_id = dX[0]\n",
    "        self.attention_mask = dX[1]\n",
    "        self.token_type_id = dX[2]\n",
    "        self.label = dY      \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_id[idx]\n",
    "        attention_masks = self.attention_mask[idx]\n",
    "        token_type_ids = self.token_type_id[idx]        \n",
    "        if self.label is None:            \n",
    "            return input_ids, attention_masks, token_type_ids            \n",
    "        else :            \n",
    "            label = self.label[idx]        \n",
    "            return input_ids, attention_masks, token_type_ids, label"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fe923be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def _read_file(input_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split('\\t')\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences, labels\n",
    "    \n",
    "def eval_input(test, token, args, pad_token_label_id, mask_padding_with_zero = True):\n",
    "    words = test.split()\n",
    "    tokens = []\n",
    "    slot_label_mask = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        word_tokens = token.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token] \n",
    "        tokens.extend(word_tokens)\n",
    "        slot_label_mask.extend([0] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > args.max_seq_len - special_tokens_count:\n",
    "        slot_label_mask = slot_label_mask[:(args.max_seq_len - special_tokens_count)]\n",
    "    slot_label_mask += [pad_token_label_id]\n",
    "    slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "    padding_length = args.max_seq_len - len(slot_label_mask)\n",
    "    slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
    "    input_id, attention_mask, token_type_id = token_generate(test, token, args.max_seq_len)\n",
    "    input_ids = torch.tensor(input_id, dtype=torch.long).reshape(1,-1)\n",
    "    attention_mask = torch.tensor(attention_mask, dtype=torch.long).reshape(1,-1)\n",
    "    token_type_ids = torch.tensor(token_type_id, dtype=torch.long).reshape(1,-1)\n",
    "    slot_label_mask = torch.tensor(slot_label_mask, dtype=torch.long).reshape(1,-1)    \n",
    "    return input_ids, attention_mask, token_type_ids, slot_label_mask    \n",
    "\n",
    "def eval_ft(test_sent, label_lst_, model_, tok, dev, args):\n",
    "    preds = None\n",
    "    pad_token = torch.nn.CrossEntropyLoss().ignore_index    \n",
    "    with torch.no_grad():\n",
    "        tmp_input, tmp_attention, tmp_token, tmp_slot = eval_input(test_sent, tok, args, pad_token)    \n",
    "        inputs = {'input_ids':tmp_input.to(device), 'attention_mask':tmp_attention.to(device), \n",
    "                  'labels' : None,\n",
    "                  'token_type_ids': tmp_token.to(device)}\n",
    "        output = model_(**inputs)\n",
    "        logits = output[0]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = np.argmax(preds, axis=2)\n",
    "        slot_label_map = {i : label for i, label in enumerate(label_lst_)}   \n",
    "    preds_list = []\n",
    "    for j in range(preds.shape[1]):\n",
    "        if tmp_slot[0,j] != pad_token:\n",
    "            preds_list.append(slot_label_map[preds[0][j]])            \n",
    "\n",
    "    line = \"\"\n",
    "    for w, p in zip(test_sent.split(), preds_list):\n",
    "        line = line + \" {}\".format(p)\n",
    "        #if p == \"O\":\n",
    "        #    line = line + w + \" \"\n",
    "        #else :\n",
    "        #    line = line + \"{}[{}] \".format(w, p)\n",
    "\n",
    "    return line#, logits\n",
    "\n",
    "def get_labels(label_path):\n",
    "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a112f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGES_sent_210804.dta', 'rb') as a_sent:\n",
    "    train_sent = pickle.load(a_sent)\n",
    "with open('LGES_sent_ner_level1_210804.dta', 'rb') as a_sent_l1:\n",
    "    train_ner_l1 = pickle.load(a_sent_l1)\n",
    "with open('LGES_label_l1_210804.dta', 'rb') as a_label1:\n",
    "    train_label_l1 = pickle.load(a_label1)\n",
    "    \n",
    "args = argparse.Namespace(  \n",
    "    max_seq_len = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be44f0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set :  605785\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = [], []\n",
    "for i in range(0, len(train_sent)): \n",
    "    sentences.append(train_sent[i])\n",
    "    labels.append(train_ner_l1[i])\n",
    "train_dict = {\"sentence\": sentences, \"label\":labels}\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "ner_begin_label = [train_label_l1.index(begin_label) for begin_label in train_label_l1 if \"B\" in begin_label]\n",
    "print(\"Train Set : \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47af8f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370.183789730072\n"
     ]
    }
   ],
   "source": [
    "s_time =time.time()\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "pad_token_label_id = 0\n",
    "cls_token_label_id = 0\n",
    "sep_token_label_id = 0\n",
    "train_inputs, train_labels = generate_input(train_df, tokenizer, ner_begin_label, 128)\n",
    "print(time.time() - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407b4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = custom_set(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e77468df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec78e59b76741948c8a0f9f12fa9d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BertForTokenClassification.from_pretrained('monologg/kobert', num_labels=len(train_label_l1))#\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, eps=1e-3)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec724ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch_loss = 0\n",
    "for epoch in tqdm(range(0, 1)):\n",
    "    s_time = time.time()\n",
    "    batch_loss = 0\n",
    "    for i, [tmp_i, tmp_a, tmp_t, tmp_l] in enumerate(train_loader):\n",
    "        tmp_input = tmp_i.to(device)\n",
    "        tmp_attention = tmp_a.to(device)\n",
    "        tmp_token = tmp_t.to(device)\n",
    "        tmp_label = tmp_l.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(input_ids=tmp_input.to(device), attention_mask=tmp_attention.to(device), \n",
    "                    token_type_ids=tmp_token.to(device), labels=tmp_label.to(device))\n",
    "        loss = outputs.loss \n",
    "        logit = np.argmax(outputs.logits.detach().cpu().numpy(), axis=2) \n",
    "        loss.backward()\n",
    "        optimizer.step()          \n",
    "        batch_loss += loss.item()\n",
    "    e_time = time.time()\n",
    "    tqdm.write('[Epoch : %d] train_loss: %.5f / Time : %f' % (epoch, batch_loss / (i+1), e_time - s_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dfde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
