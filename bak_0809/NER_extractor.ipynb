{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1644a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql, os, copy, json, time, openpyxl\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import re\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from kss import kss\n",
    "from pororo import Pororo\n",
    "from konlpy.tag import Mecab\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad35f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4bc90d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def token_generate(sent, tok, MAX_LEN):\n",
    "    encode_dict = tok.encode_plus(text=sent, \n",
    "                                    add_special_tokens=True, max_length=MAX_LEN,\n",
    "                                    return_token_type_ids=True, padding='max_length', #pad_to_max_length=True,\n",
    "                                    return_attention_mask=True, truncation=True) # return_tensors='pt',\n",
    "\n",
    "    input_id = encode_dict['input_ids']\n",
    "    attention_mask = encode_dict['attention_mask']\n",
    "    token_type_id = encode_dict['token_type_ids']\n",
    "    return input_id, attention_mask, token_type_id \n",
    "\n",
    "def _read_file(input_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split('\\t')\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences, labels\n",
    "    \n",
    "def eval_input(test, token, args, pad_token_label_id, mask_padding_with_zero = True):\n",
    "\n",
    "    words = test.split()\n",
    "    tokens = []\n",
    "    slot_label_mask = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        word_tokens = token.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "        tokens.extend(word_tokens)\n",
    "        slot_label_mask.extend([0] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "    # Account for [CLS] and [SEP]\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > args.max_seq_len - special_tokens_count:\n",
    "        slot_label_mask = slot_label_mask[:(args.max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # Add [SEP] token\n",
    "    slot_label_mask += [pad_token_label_id]\n",
    "    slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "    padding_length = args.max_seq_len - len(slot_label_mask)\n",
    "    slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
    "    input_id, attention_mask, token_type_id = token_generate(test, token, args.max_seq_len)\n",
    "\n",
    "    input_ids = torch.tensor(input_id, dtype=torch.long).reshape(1,-1)\n",
    "    attention_mask = torch.tensor(attention_mask, dtype=torch.long).reshape(1,-1)\n",
    "    token_type_ids = torch.tensor(token_type_id, dtype=torch.long).reshape(1,-1)\n",
    "    slot_label_mask = torch.tensor(slot_label_mask, dtype=torch.long).reshape(1,-1)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, slot_label_mask    \n",
    "\n",
    "def eval_ft(test_sent, label_lst, model_, tok, dev, args):\n",
    "    preds = None\n",
    "    pad_token = torch.nn.CrossEntropyLoss().ignore_index    \n",
    "    with torch.no_grad():\n",
    "        tmp_input, tmp_attention, tmp_token, tmp_slot = eval_input(test_sent, tok, args, pad_token)    \n",
    "        inputs = {'input_ids':tmp_input.to(device), 'attention_mask':tmp_attention.to(dev), \n",
    "                  'labels' : None,\n",
    "                  'token_type_ids': tmp_token.to(device)}\n",
    "        output = model_(**inputs)\n",
    "        logits = output[0]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = np.argmax(preds, axis=2)\n",
    "        slot_label_map = {i : label for i, label in enumerate(label_lst)}   \n",
    "    preds_list = []\n",
    "    for j in range(preds.shape[1]):\n",
    "        if tmp_slot[0,j] != pad_token:\n",
    "            preds_list.append(slot_label_map[preds[0][j]])            \n",
    "\n",
    "    line = \"\"\n",
    "    for w, p in zip(test_sent.split(), preds_list):\n",
    "        line = line + \" {}\".format(p)\n",
    "        #if p == \"O\":\n",
    "        #    line = line + w + \" \"\n",
    "        #else :\n",
    "        #    line = line + \"{}[{}] \".format(w, p)\n",
    "\n",
    "    return line\n",
    "\n",
    "def get_labels(label_path):\n",
    "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]\n",
    "\n",
    "def _call_db_info():\n",
    "    return pymysql.connect(\n",
    "        host = '183.111.204.69',\n",
    "        port= 13306,\n",
    "        user = 'newsbot1',\n",
    "        password='lgensol2020!',\n",
    "        db = 'trend',\n",
    "        charset = 'utf8')\n",
    "def extract_parenthese(str):\n",
    "    items_lst = re.findall('\\(([^)]+)', str) #extracts string in () \n",
    "    newList = [x for x in items_lst if len(x)>=2] # more than 2\n",
    "    return newList\n",
    "\n",
    "def extract_quotes(str):\n",
    "    items_lst = re.findall('\"([^\"]*)\"', str)\n",
    "    return items_lst\n",
    "\n",
    "def parentheses_(tmp_input_sent):    \n",
    "    tmp_input_sent = re.sub(pattern='\\(+', repl=' ', string=tmp_input_sent)#tmp_input_sent = re.sub(pattern='\\(\\(', repl='\\(', string=tmp_input_sent)\n",
    "    tmp_input_sent = re.sub(pattern='\\)+', repl=' ', string=tmp_input_sent)#tmp_input_sent = re.sub(pattern='\\)\\)', repl='\\)', string=tmp_input_sent)\n",
    "    tmp_input_sent = re.sub(pattern=' +', repl=' ', string=tmp_input_sent)\n",
    "    input_sent = re.sub(pattern='\\\\\\\\',   repl='', string=tmp_input_sent)\n",
    "    return input_sent\n",
    "    '''\n",
    "    tmp_sent1, tmp_sent2 = [], []    \n",
    "    s_re = re.compile('\\(')#tmp_sentence[25])#.match('\\(')\n",
    "    e_re = re.compile('\\)')\n",
    "    s_m = [(m.start(0), m.end(0)) for m in s_re.finditer(input_sent)]#tmp_sentence[25])]\n",
    "    e_m = [(m.start(0), m.end(0)) for m in e_re.finditer(input_sent)]#tmp_sentence[25])]\n",
    "    m = []\n",
    "    for i in range(0, len(s_m)):\n",
    "        if s_m[i][1] < e_m[i][0]:\n",
    "            m.append((s_m[i], e_m[i]))\n",
    "        else:\n",
    "            for j in range(i+1, len(e_m)):\n",
    "                if s_m[i][1] < e_m[j][0]:\n",
    "                    m.append((s_m[i], e_m[j]))\n",
    "                    break;                   \n",
    "\n",
    "    if len(m) > 0:\n",
    "        for i in range(0, len(m)):\n",
    "            if i == 0:\n",
    "                tmp_sent1.append(input_sent[:m[i][0][0]])\n",
    "            else :\n",
    "                tmp_sent1.append(input_sent[m[(i-1)][1][1]:m[i][0][0]])                        \n",
    "            tmp_sent2.append(input_sent[m[i][0][1]:m[i][1][0]])\n",
    "        tmp_sent1.append(input_sent[m[-1][1][1]:])    \n",
    "        return ' '.join(tmp_sent1 + tmp_sent2)\n",
    "    else:\n",
    "        return input_sent\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f9371a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tmp_file_raw = openpyxl.load_workbook('./20210628_레퍼런스2.4.xlsx')\n",
    "shee_name = tmp_file_raw.sheetnames\n",
    "ref_dic = pd.DataFrame()\n",
    "for f in range(0, 2):\n",
    "    tmp_file_pd = pd.DataFrame(tmp_file_raw[shee_name[f]].values).copy()\n",
    "    tmp_col_name1 = list(tmp_file_pd.iloc[0,0:])\n",
    "    tmp_file = tmp_file_pd.iloc[1:,:].copy().reset_index(drop=True)\n",
    "    tmp_file.columns = tmp_col_name1\n",
    "    tmp_ref_dic = tmp_file[['대분류','명칭']]\n",
    "    ref_dic = pd.concat((ref_dic, tmp_ref_dic))\n",
    "'''\n",
    "ref_dic = pd.read_csv('ref_dic.csv')#ref_dic.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff8e493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BertForTokenClassification.from_pretrained('./kobert')\n",
    "token = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "label_lst = get_labels('./kobert/label.txt')\n",
    "args = argparse.Namespace(max_seq_len = 128)\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a71486",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = _call_db_info()\n",
    "curs = conn.cursor()\n",
    "tmp_insert_sql = \"select * from word_dic\"\n",
    "curs.execute(tmp_insert_sql)        \n",
    "tmp_rslt = pd.DataFrame(curs.fetchall())\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01aef8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = _call_db_info()\n",
    "curs = conn.cursor()\n",
    "tmp_insert_sql = \"select * from content\" #where date >=20210714\"#\"select * from word_dic\"\n",
    "curs.execute(tmp_insert_sql)        \n",
    "tmp_article = pd.DataFrame(curs.fetchall())\n",
    "conn.commit()\n",
    "conn.close()\n",
    "tmp_article.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f103081a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "step0_ptn= '[\\'\\‘\\’]'\n",
    "step1_ptn= '[\\u00a0\\u3000①②③④⑤⑥⑦⑧⑨⑩』◦※→®↑↓‣★▶■△◇◆▲○●\\{\\}\\[\\]\\/?,+;:‧·ᆞ…》ⓒ|*~`\\\"\"“”!^_<>@\\#&\\\\\\=\\'\\n]'     \n",
    "step2_ptn= '[\\.]' \n",
    "article_sent = []\n",
    "article_ner_l1, article_ner_l2 = [], []\n",
    "a = 644\n",
    "print('Article %s'%a)\n",
    "tmp_a = re.sub(pattern=step0_ptn, repl='\\\"', string=tmp_article.iloc[a, 4])#.replace('\\'‘’', '\\\"')\n",
    "tmp_a = re.sub(pattern=step1_ptn, repl='', string=tmp_a)\n",
    "tmp_sentence = kss.split_sentences(tmp_a)\n",
    "tmp_sentence = [re.sub(pattern=step2_ptn, repl='', string=s) for s in tmp_sentence]\n",
    "tmp_sentence = [parentheses_(s) for s in tmp_sentence] \n",
    "\n",
    "ner_sent, ner_tag_l1, ner_tag_l2 = [], [], []\n",
    "pos_set = ref_dic['명칭'].values.tolist()\n",
    "\n",
    "sent = 11\n",
    "tmp_ner = []\n",
    "tmp_sent = re.sub(pattern=' +', repl=' ', string=tmp_sentence[sent])\n",
    "sent_split = tmp_sent.split(' ')\n",
    "tmp_sent_n, tmp_sent_a = divmod(len(sent_split), 30)\n",
    "if tmp_sent_a == 0:\n",
    "    sent_n = tmp_sent_n\n",
    "else :\n",
    "    sent_n = tmp_sent_n + 1\n",
    "\n",
    "if len(sent_split) > 30 :        \n",
    "    sent_ = []\n",
    "    for i in range(0, sent_n):            \n",
    "        if i < sent_n:\n",
    "            sent_.append(sent_split[(i*30):((i+1)*30)])\n",
    "        else:\n",
    "            sent_.append(sent_split[(i*30):])\n",
    "    for i in range(0, sent_n):            \n",
    "        tmp_ner.append(eval_ft(' '.join(sent_[i]), label_lst, model, token, device, args))                \n",
    "    tmp_ner = (' ').join(tmp_ner)\n",
    "else:    \n",
    "    tmp_ner = eval_ft(tmp_sent, label_lst, model, token, device, args).strip() \n",
    "\n",
    "tmp_mecab_pos = [mecab.pos(w) for w in sent_split]\n",
    "tmp_ner = tmp_ner = [t for t in tmp_ner.split(' ') if len(t) >0]#tmp_ner.split(' ')\n",
    "tmp_ner_l1 = ['O'] * len(tmp_ner)#tmp_ner.copy()\n",
    "tmp_ner_l2 = ['O'] * len(tmp_ner)#tmp_ner.copy()\n",
    "for k in range(0 ,len(tmp_ner)):    \n",
    "    m_pos = [i for i, tmp_me in enumerate(tmp_mecab_pos[k]) if tmp_me[1] == 'NNP']\n",
    "    if len(m_pos) > 0:\n",
    "        for l in range(0, len(m_pos)):#tmp_mecab_pos[4][m_pos[0]][0]\n",
    "            if tmp_mecab_pos[k][m_pos[l]][0] in pos_set:\n",
    "                ner_ind = pos_set.index(tmp_mecab_pos[k][m_pos[l]][0])                    \n",
    "                tmp_ner_l1[k] = ref_dic['대분류'].loc[ner_ind]\n",
    "                tmp_ner_l2[k] = ref_dic['중분류'].loc[ner_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d54695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0\n"
     ]
    }
   ],
   "source": [
    "step0_ptn= '[\\'\\‘\\’]'\n",
    "step1_ptn= '[\\u00a0\\u3000①②③④⑤⑥⑦⑧⑨⑩』◦※→®↑↓‣★▶■△◇◆▲○●\\{\\}\\[\\]\\/?,+;:‧·ᆞ…》ⓒ|*~`\\\"\"“”!^_<>@\\#&\\\\\\=\\'\\n]'     \n",
    "step2_ptn= '[\\.]' \n",
    "article_sent = []\n",
    "article_ner_l1, article_ner_l2 = [], []\n",
    "for a in range(0, tmp_article.shape[0]):    \n",
    "    #print('Article %s'%a)\n",
    "    tmp_a = re.sub(pattern=step0_ptn, repl='\\\"', string=tmp_article.iloc[a, 4])#.replace('\\'‘’', '\\\"')\n",
    "    tmp_a = re.sub(pattern=step1_ptn, repl='', string=tmp_a)\n",
    "    tmp_sentence = kss.split_sentences(tmp_a)\n",
    "    tmp_sentence = [re.sub(pattern=step2_ptn, repl='', string=s) for s in tmp_sentence]\n",
    "    tmp_sentence = [parentheses_(s) for s in tmp_sentence] \n",
    "\n",
    "    ner_sent, ner_tag_l1, ner_tag_l2 = [], [], []\n",
    "    pos_set = ref_dic['명칭'].values.tolist()\n",
    "\n",
    "    for sent in range(0, len(tmp_sentence)):\n",
    "        #print(sent)\n",
    "        tmp_ner = []\n",
    "        tmp_sent = re.sub(pattern=' +', repl=' ', string=tmp_sentence[sent])\n",
    "        sent_split = tmp_sent.split(' ')\n",
    "        tmp_sent_n, tmp_sent_a = divmod(len(sent_split), 30)\n",
    "        if tmp_sent_a == 0:\n",
    "            sent_n = tmp_sent_n\n",
    "        else :\n",
    "            sent_n = tmp_sent_n + 1\n",
    "\n",
    "        if len(sent_split) > 30 :        \n",
    "            sent_ = []\n",
    "            for i in range(0, sent_n):            \n",
    "                if i < sent_n:\n",
    "                    sent_.append(sent_split[(i*30):((i+1)*30)])\n",
    "                else:\n",
    "                    sent_.append(sent_split[(i*30):])\n",
    "            for i in range(0, sent_n):            \n",
    "                tmp_ner.append(eval_ft(' '.join(sent_[i]), label_lst, model, token, device, args))                \n",
    "            tmp_ner = (' ').join(tmp_ner)\n",
    "        else:    \n",
    "            tmp_ner = eval_ft(tmp_sent, label_lst, model, token, device, args).strip() \n",
    "\n",
    "        tmp_mecab_pos = [mecab.pos(w) for w in sent_split]\n",
    "        tmp_ner = tmp_ner = [t for t in tmp_ner.split(' ') if len(t) >0]#tmp_ner.split(' ')\n",
    "        tmp_ner_l1 = ['O'] * len(tmp_ner)#tmp_ner.copy()\n",
    "        tmp_ner_l2 = ['O'] * len(tmp_ner)#tmp_ner.copy()\n",
    "        for k in range(0 ,len(tmp_ner)):    \n",
    "            m_pos = [i for i, tmp_me in enumerate(tmp_mecab_pos[k]) if tmp_me[1] == 'NNP']\n",
    "            if len(m_pos) > 0:\n",
    "                for l in range(0, len(m_pos)):#tmp_mecab_pos[4][m_pos[0]][0]\n",
    "                    if tmp_mecab_pos[k][m_pos[l]][0] in pos_set:\n",
    "                        ner_ind = pos_set.index(tmp_mecab_pos[k][m_pos[l]][0])                    \n",
    "                        tmp_ner_l1[k] = ref_dic['대분류'].loc[ner_ind]\n",
    "                        tmp_ner_l2[k] = ref_dic['중분류'].loc[ner_ind]\n",
    "        ner_sent.append((' ').join(sent_split))\n",
    "        ner_tag_l1.append((' ').join(tmp_ner_l1))\n",
    "        ner_tag_l2.append((' ').join(tmp_ner_l2))\n",
    "    article_sent.extend(ner_sent)\n",
    "    article_ner_l1.extend(ner_tag_l1)\n",
    "    article_ner_l2.extend(ner_tag_l2)\n",
    "    if a % 100 == 0:\n",
    "        print('Article %s'%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d2b6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent : 569\n",
      "Sent tagging : 569\n"
     ]
    }
   ],
   "source": [
    "print(\"Sent : %d\"%len(article_sent))\n",
    "print(\"Sent tagging : %d\"%len(article_ner_l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cd34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGES_sent.dta', 'wb') as a_sent:\n",
    "    pickle.dump(article_sent, a_sent)\n",
    "with open('LGES_sent_ner_level1.dta', 'wb') as a_sent_l1:\n",
    "    pickle.dump(article_ner_l1, a_sent_l1)\n",
    "with open('LGES_sent_ner_level2.dta', 'wb') as a_sent_l2:\n",
    "    pickle.dump(article_ner_l2, a_sent_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37ed7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ner_l1_ = [x for a_ner in article_ner_l1 for x in a_ner.split(' ') if x != 'O' and len(x) > 0 ]\n",
    "label_ner_l2_ = [x for a_ner in article_ner_l2 for x in a_ner.split(' ') if x != 'O' and len(x) > 0]\n",
    "tmp_label_ner_l1 = list(set(label_ner_l1_))\n",
    "tmp_label_ner_l2 = list(set(label_ner_l2_))\n",
    "tmp_label_ner_l1.sort()\n",
    "tmp_label_ner_l2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6000b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ner_l1 = ['UNK']# + tmp_label_ner_l1\n",
    "label_ner_l2 = ['UNK']# + tmp_label_ner_l2\n",
    "for i in tmp_label_ner_l1:\n",
    "    label_ner_l1.extend([i+'-B', i+'-I'])\n",
    "for i in tmp_label_ner_l2:\n",
    "    label_ner_l2.extend([i+'-B', i+'-I'])\n",
    "with open('LGES_label_l1.dta', 'wb') as a_label1:\n",
    "    pickle.dump(label_ner_l1, a_label1)\n",
    "with open('LGES_label_l2.dta', 'wb') as a_label2:\n",
    "    pickle.dump(label_ner_l2, a_label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad94edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " '기관-B',\n",
       " '기관-I',\n",
       " '기술-B',\n",
       " '기술-I',\n",
       " '기업-B',\n",
       " '기업-I',\n",
       " '동향-B',\n",
       " '동향-I',\n",
       " '서비스-B',\n",
       " '서비스-I',\n",
       " '소재-B',\n",
       " '소재-I',\n",
       " '전지-B',\n",
       " '전지-I',\n",
       " '제품-B',\n",
       " '제품-I',\n",
       " '트렌드-B',\n",
       " '트렌드-I']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ner_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a57dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
