{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b338fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql, os, copy, json, time, openpyxl, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import  Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63a34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "from tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8f2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_generate(sent, tok, MAX_LEN):\n",
    "    encode_dict = tok.encode_plus(text=sent, \n",
    "                                    add_special_tokens=True, max_length=MAX_LEN,\n",
    "                                    return_token_type_ids=True, padding='max_length', #pad_to_max_length=True,\n",
    "                                    return_attention_mask=True, truncation=True) # return_tensors='pt',\n",
    "\n",
    "    input_id = encode_dict['input_ids']\n",
    "    attention_mask = encode_dict['attention_mask']\n",
    "    token_type_id = encode_dict['token_type_ids']\n",
    "    return input_id, attention_mask, token_type_id  \n",
    "\n",
    "def convert_label(words, labels_idx, tok,  ner_b_label, max_seq_len):\n",
    "    cls_token = tok.cls_token\n",
    "    sep_token = tok.sep_token\n",
    "    unk_token = tok.unk_token\n",
    "    pad_token_id = tok.pad_token_id\n",
    "    tokens = []\n",
    "    labels_ids = []\n",
    "    \n",
    "    for word, slot_label in zip(words, labels_idx):\n",
    "        w_token = tok.tokenize(word)\n",
    "        if not w_token:\n",
    "            w_token = [unk_token]\n",
    "        tokens.extend(w_token)\n",
    "        #labels_ids.extend([int(slot_label)] * len(w_token))\n",
    "        if int(slot_label) in ner_b_label:\n",
    "            labels_ids.extend([int(slot_label)] + [int(slot_label) + 1] * (len(w_token)-1))\n",
    "        else:\n",
    "            labels_ids.extend([int(slot_label)] * len(w_token))\n",
    "            \n",
    "    special_tokens_cnt = 2\n",
    "    if len(labels_ids) > max_seq_len - special_tokens_cnt:\n",
    "        labels_ids = labels_ids[:(max_seq_len - special_tokens_cnt)]\n",
    "        \n",
    "    labels_ids += [sep_token_label_id]\n",
    "    labels_ids = [cls_token_label_id] + labels_ids\n",
    "    \n",
    "    padding_len = max_seq_len - len(labels_ids)\n",
    "    labels_ids = labels_ids + ([pad_token_label_id] * padding_len)\n",
    "    \n",
    "    return labels_ids\n",
    "\n",
    "def generate_input(df, tok, ner_b_label, max_seq_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, data in enumerate(df[['sentence', 'label']].values):\n",
    "        sentence, labels = data\n",
    "        words = sentence.split()\n",
    "        labels = labels.split()\n",
    "        labels_idx = []\n",
    "        \n",
    "        for l in labels:    \n",
    "            labels_idx.append(train_label_l1.index(l+'-B') if l+'-B' in train_label_l1 else train_label_l1.index(\"UNK\"))\n",
    "        \n",
    "        input_id, attention_mask, token_type_id = token_generate(sentence, tok, max_seq_len)\n",
    "        convert_label_id = convert_label(words, labels_idx, tok, ner_b_label, max_seq_len)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_list.append(convert_label_id)\n",
    "        \n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_list = np.asarray(label_list, dtype=int)\n",
    "    inputs = (input_ids, attention_masks, token_type_ids)\n",
    "    \n",
    "    return inputs, label_list\n",
    "\n",
    "class custom_set(Dataset):\n",
    "    def __init__(self, dX, dY=None):\n",
    "        self.input_id = dX[0]\n",
    "        self.attention_mask = dX[1]\n",
    "        self.token_type_id = dX[2]\n",
    "        self.label = dY\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_id[idx]\n",
    "        attention_masks = self.attention_mask[idx]\n",
    "        token_type_ids = self.token_type_id[idx]\n",
    "        \n",
    "        if self.label is None:            \n",
    "            return input_ids, attention_masks, token_type_ids            \n",
    "        else :            \n",
    "            label = self.label[idx]        \n",
    "            return input_ids, attention_masks, token_type_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543b1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_file(input_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split('\\t')\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences, labels\n",
    "    \n",
    "def eval_input(test, token, args, pad_token_label_id, mask_padding_with_zero = True):\n",
    "\n",
    "    words = test.split()\n",
    "    tokens = []\n",
    "    slot_label_mask = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        word_tokens = token.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "        tokens.extend(word_tokens)\n",
    "        slot_label_mask.extend([0] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "    # Account for [CLS] and [SEP]\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > args.max_seq_len - special_tokens_count:\n",
    "        slot_label_mask = slot_label_mask[:(args.max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # Add [SEP] token\n",
    "    slot_label_mask += [pad_token_label_id]\n",
    "    slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "    padding_length = args.max_seq_len - len(slot_label_mask)\n",
    "    slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
    "    input_id, attention_mask, token_type_id = token_generate(test, token, args.max_seq_len)\n",
    "\n",
    "    input_ids = torch.tensor(input_id, dtype=torch.long).reshape(1,-1)\n",
    "    attention_mask = torch.tensor(attention_mask, dtype=torch.long).reshape(1,-1)\n",
    "    token_type_ids = torch.tensor(token_type_id, dtype=torch.long).reshape(1,-1)\n",
    "    slot_label_mask = torch.tensor(slot_label_mask, dtype=torch.long).reshape(1,-1)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, slot_label_mask    \n",
    "\n",
    "def eval_ft(test_sent, label_lst_, model_, tok, dev, args):\n",
    "    preds = None\n",
    "    pad_token = torch.nn.CrossEntropyLoss().ignore_index    \n",
    "    with torch.no_grad():\n",
    "        tmp_input, tmp_attention, tmp_token, tmp_slot = eval_input(test_sent, tok, args, pad_token)    \n",
    "        inputs = {'input_ids':tmp_input.to(device), 'attention_mask':tmp_attention.to(device), \n",
    "                  'labels' : None,\n",
    "                  'token_type_ids': tmp_token.to(device)}\n",
    "        output = model_(**inputs)\n",
    "        logits = output[0]\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = np.argmax(preds, axis=2)\n",
    "        slot_label_map = {i : label for i, label in enumerate(label_lst_)}   \n",
    "    preds_list = []\n",
    "    for j in range(preds.shape[1]):\n",
    "        if tmp_slot[0,j] != pad_token:\n",
    "            preds_list.append(slot_label_map[preds[0][j]])            \n",
    "\n",
    "    line = \"\"\n",
    "    for w, p in zip(test_sent.split(), preds_list):\n",
    "        line = line + \" {}\".format(p)\n",
    "        #if p == \"O\":\n",
    "        #    line = line + w + \" \"\n",
    "        #else :\n",
    "        #    line = line + \"{}[{}] \".format(w, p)\n",
    "\n",
    "    return line#, logits\n",
    "\n",
    "def get_labels(label_path):\n",
    "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55b8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGES_sent_210804.dta', 'rb') as a_sent:\n",
    "    train_sent = pickle.load(a_sent)\n",
    "with open('LGES_sent_ner_level1_210804.dta', 'rb') as a_sent_l1:\n",
    "    train_ner_l1 = pickle.load(a_sent_l1)\n",
    "with open('LGES_label_l1_210804.dta', 'rb') as a_label1:\n",
    "    train_label_l1 = pickle.load(a_label1)\n",
    "    \n",
    "args = argparse.Namespace(  \n",
    "    max_seq_len = 128\n",
    ")\n",
    "#args.model_name_or_path = MODEL_PATH_MAP[args.model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71abc141",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_ratio = []\n",
    "for k in range(0, len(train_ner_l1)):\n",
    "    #print(k)\n",
    "    sent_total_cnt = len(train_ner_l1[k].split(' '))\n",
    "    sent_o_cnt = np.sum([1 if x =='O' else 0 for x in train_ner_l1[k].split(' ') ])\n",
    "    o_ratio.append( sent_o_cnt  / sent_total_cnt)\n",
    "#select_sent_id = [i for i,z in enumerate(o_ratio) if z < 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be85c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set :  605785\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = [], []\n",
    "for i in range(0, len(train_sent)): #select_sent_id:#\n",
    "    sentences.append(train_sent[i])#.split(' '))\n",
    "    labels.append(train_ner_l1[i])#.split(' '))\n",
    "train_dict = {\"sentence\": sentences, \"label\":labels}\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "ner_begin_label = [train_label_l1.index(begin_label) for begin_label in train_label_l1 if \"B\" in begin_label]\n",
    "print(\"Train Set : \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fccf790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37fa5a5bab046918edee224a417e895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/371k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb24de15ecc497494f5e056993e48be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/77.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08b627ce1814340910672c5472e0107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8eff0352aee452d94c7178bb5e5c862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371.885374546051\n"
     ]
    }
   ],
   "source": [
    "s_time =time.time()\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "pad_token_label_id = 0\n",
    "cls_token_label_id = 0\n",
    "sep_token_label_id = 0\n",
    "train_inputs, train_labels = generate_input(train_df, tokenizer, ner_begin_label, 128)\n",
    "print(time.time() - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95302f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = custom_set(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0999d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BertForTokenClassification.from_pretrained('monologg/kobert', num_labels=len(train_label_l1))#\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, eps=1e-3)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f41d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [05:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 0] train_loss: 0.93507 / Time : 302.104509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:02<00:00, 302.68s/it]\n"
     ]
    }
   ],
   "source": [
    "save_folder = './save/'\n",
    "if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "model.train()\n",
    "epoch_loss = 0\n",
    "for epoch in tqdm(range(0, 1)):\n",
    "    #tqdm_loader = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "    s_time = time.time()\n",
    "    batch_loss = 0\n",
    "    for i, [tmp_i, tmp_a, tmp_t, tmp_l] in enumerate(train_loader):\n",
    "        tmp_input = tmp_i.to(device)\n",
    "        tmp_attention = tmp_a.to(device)\n",
    "        tmp_token = tmp_t.to(device)\n",
    "        tmp_label = tmp_l.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(input_ids=tmp_input.to(device), attention_mask=tmp_attention.to(device), \n",
    "                    token_type_ids=tmp_token.to(device), labels=tmp_label.to(device))\n",
    "        loss = outputs.loss \n",
    "        logit = np.argmax(outputs.logits.detach().cpu().numpy(), axis=2) \n",
    "        loss.backward()\n",
    "        optimizer.step()          \n",
    "        batch_loss += loss.item()\n",
    "    e_time = time.time()\n",
    "    tqdm.write('[Epoch : %d] train_loss: %.5f / Time : %f' % (epoch, batch_loss / (i+1), e_time - s_time))    \n",
    "    if epoch % 10 == 0:\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        model_to_save.save_pretrained(save_folder+'epoch'+str(epoch))#,state_dict=save_folder+'epoch'+str(epoch))\n",
    "    #torch.save(args, os.path.join(save_folder+'epoch'+str(epoch)+'/train_args.bin'))\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(save_folder)\n",
    "#torch.save(os.path.join('./save/train_args.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f3c2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
